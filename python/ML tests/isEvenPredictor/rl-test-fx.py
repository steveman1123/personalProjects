#original code generated by chatgpt, modified
#attempt to test a pattern for if the next will be increase or decrease than the previous (similar to the increase/decrease version)


#TODO: accuracy is low due to "compressing" too many info bits. Right now, I'm asking for direction (+/-) and magnitude (mindiff), but can only transfer 1 bit of info (right now just direction). Need to figure out how to reward or scale rewards with correct direction and magnitudes (could just make it equal to the data?) - accuracy goes way up if mindiff=0

#TODO: add comments for data flow

import random,json

# Define the dataset
datafile = "datasets/eurusd-minute-close.txt"
with open(datafile,'r') as f:
  data = f.readlines()
  data = [float(e) for e in data]

datalen = 1000 #length of dataset to use
offset = random.randint(0,len(data)-datalen) #window of the overall dataset to use
data = data[-(offset+datalen):-offset] #subset of data to use

print(datafile)
print("dataset size:",len(data))

#minimum difference between data points to reward (instead of strictly greater than or less than, it must be greather than or less than this amount difference (eg if this is 0.1, previous point is 1.0, then the next point must be 0.9 or 1.1 in order to be rewarded))
mindiff = 0.0005 #0.0001=1 pip

# Initialize the Q-table
q_table = {}
for i in range(len(data)):
  q_table[i] = {'increase': 0, 'decrease': 0, 'no-change':0}

# Set the hyperparameters
alpha = 0.1
gamma = 0.6
epsilon = 0.1 #liklihood of making a random choice (opposed to an informed choice)

# Define the reward function
def reward(state, action):
  if action == 'increase' and data[state+1] >= data[state]+mindiff :
    return 1
  elif action == 'decrease' and data[state+1] <= data[state]-mindiff :
    return 1
  elif action == 'no-change' and data[state]-mindiff<data[state+1]<data[state]+mindiff:
    return 1
  else:
    return 0

# Run the Q-learning algorithm
episodes = 500 #number of episodes to train

print("training progress:")
print("_"*100)
for i in range(episodes):
  if(int(i/(episodes/100))==i/(episodes/100)): print("#",end="",flush=True)

  state = 0
  done = False

  while not done:
    #randomly choose an increase or decrease number 10% of the time
    if random.uniform(0, 1) < epsilon:
      action = random.choice(['increase', 'decrease','no-change'])
    else:
      #get the choice values of the current state
      q_values = q_table[state]
      #choose the most likely value as the action
      action = max(q_values, key=q_values.get)

    #give us the reward!
    r = reward(state, action)
    

    #move on to the next state
    next_state = state + 1
    
    
    if next_state == len(data) - 1:
      done = True
      q_table[state][action] += alpha * (r - q_table[state][action])
    else:
      q_next_values = q_table[next_state]
      max_q_next_value = max(q_next_values.values())
      q_table[state][action] += alpha * (r + gamma * max_q_next_value - q_table[state][action])
      state = next_state

# Use the learned Q-table to predict the next number
print()

latesttestpoint = q_table[len(data)-2]
if(max(latesttestpoint,key=latesttestpoint.get)=='increase'):
  print(f'The next number ({data[-1]}) should be increase (than {data[-2]})')
elif(max(latesttestpoint,key=latesttestpoint.get)=='decrease'):
  print(f'The next number ({data[-1]}) should be decrease (than {data[-2]})')
else:
  print(f'The next number ({data[-1]}) should have a small change (between {round(data[-2]-mindiff,5)} and {round(data[-2]+mindiff,5)})')

# if q_table[len(data)-2]['increase'] > q_table[len(data)-2]['decrease']:
#   print(f'The next number ({data[-1]}) should be increase (than {data[-2]})')
# else:
#   print(f'The next number ({data[-1]}) should be decrease (than {data[-2]})')


'''
In this example, we first define the dataset as a list of integers. We then initialize the Q-table as a dictionary, where each state-action pair has an initial Q-value of 0. We set the hyperparameters for the Q-learning algorithm, including the learning rate (alpha), discount factor (gamma), and exploration rate (epsilon). We also define a reward function that returns 1 if the predicted action is correct and 0 otherwise.

We then run the Q-learning algorithm for a fixed number of iterations, where each iteration represents a complete pass through the dataset. In each iteration, we start at the beginning of the dataset (state=0) and take actions according to an epsilon-greedy policy. We update the Q-values using the Bellman equation, which incorporates the immediate reward and the estimated value of the next state-action pair. Once we reach the end of the dataset, we terminate the episode and update the Q-value for the final state-action pair.

Finally, we use the learned Q-table to predict the next number in the dataset by selecting the action with the highest Q-value for the second-to-last state in the dataset. If the predicted action is 'increase', we print 'The next number is increase'. Otherwise, we print 'The next number is decrease'.
'''





predicted_increase = [int(max(q_table[e],key=q_table[e].get)=='increase') for e in q_table][:-1]
actual_increase = [int(data[i]>=data[i-1]+mindiff) for i in range(len(data[1:]))]

predicted_decrease = [int(max(q_table[e],key=q_table[e].get)=='decrease') for e in q_table][:-1]
actual_decrease = [int(data[i]<=data[i-1]-mindiff) for i in range(len(data[1:]))]

predicted_no_change = [int(max(q_table[e],key=q_table[e].get)=='no-change') for e in q_table][:-1]
actual_no_change = [int((data[i-1]-mindiff)<(data[i])<(data[i-1]+mindiff)) for i in range(len(data[1:]))]



correct_decrease_predictions = sum([predicted_decrease[i]==actual_decrease[i+1] for i in range(len(predicted_decrease)-1)])
total_decrease_predictions = len(predicted_decrease)-1

correct_increase_predictions = sum([predicted_increase[i]==actual_increase[i+1] for i in range(len(predicted_increase)-1)])
total_increase_predictions = len(predicted_increase)-1

correct_no_change_predictions = sum([predicted_no_change[i]==actual_no_change[i+1] for i in range(len(predicted_no_change)-1)])
total_no_change_predictions = len(predicted_no_change)-1


print("correct_decrease_predictions:",correct_decrease_predictions,"total_decrease_predictions:",total_decrease_predictions,sep="\t")
print("correct_increase_predictions:",correct_increase_predictions,"total_increase_predictions:",total_increase_predictions,sep="\t")
print("correct_no_change_predictions:",correct_no_change_predictions,"total_no_change_predictions:",total_no_change_predictions,sep="\t")
print("avg accuracy:",round(sum([correct_increase_predictions/total_increase_predictions,correct_decrease_predictions/total_decrease_predictions,correct_no_change_predictions/total_no_change_predictions])/3,5))